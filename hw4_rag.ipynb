{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0e4f49-0952-4e29-8abf-eef049810385",
   "metadata": {},
   "source": [
    "# COMS W4705 - Homework 4\n",
    "## Question Answering with Retrieval Augmented Generation\n",
    "\n",
    "Anubhav Jangra \\<aj3228@columbia.edu\\>, Emile Al-Billeh \\<ea3048@columbia.edu\\>, Daniel Bauer \\<bauer@cs.columbia.edu\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adabf07-f8d4-4e8c-82d5-1a89d3656dd4",
   "metadata": {},
   "source": [
    "In this assignment, you will use a pretrained LLM for question answering on a subset of the Stanford QA Dataset (SQuAD). Here is an example question from SQuAD: \n",
    "\n",
    "> *Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?*\n",
    "\n",
    "Specific domain knowledge to answer questions like this may not be available in the data that the LLM was pre-trained on. As a result, if we simply prompt the the LLM to answer this question, it may tell us that it does not know the answer, or worse, it may hallucinate an incorrect answer. Even if we are lucky and the LLM has have enough information to answer this question from pre-training, but the information may be outdated (the headmaster is likely to change from time to time). \n",
    "\n",
    "Luckily, SQuAD provides a context snippet for each question that may contain the answer, such as\n",
    "\n",
    "> *The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. **The school's headmaster, history professor Juan Pedro Toni**, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.*\n",
    "\n",
    "If we include the context as part of the prompt to the LLM, the model should be able to correctly answer the question (SQuAD contains \"unanswerable questions\", for which the provided context does not provide sufficient information to answer the question -- we will ignore these for the purpose of this assignment).\n",
    "\n",
    "We will consider a scenario in which we don't know which context belongs to which question and we will use **Retrieval Augmented Generation (RAG)** techniques to identify the relevant context from the set of all available contexts. \n",
    "\n",
    "Specifically we will experiment with the following systems: \n",
    "\n",
    "* A baseline \"vanilla QA\" system in which we try to answer the question without any additional context (i.e. using the pre-trained LLM only).\n",
    "* An \"oracle\" system, in which we provide the correct context for each question. This establishes an upper bound for the retrieval approaches. \n",
    "* Two different approaches for retrieving relevant contexts:\n",
    "  * based on token overlap between the question and each context.\n",
    "  * based on cosine similarity between question embeddings and candidate context embeddings (obtained using BERT).\n",
    "    \n",
    "We will evaluate each system using a number of metrics commonly used for QA tasks: \n",
    "* Exact Match (EM), which measures the percentage of predictions that exactly match the ground truth answers.\n",
    "* F1 score, measured on the token overlap between the predicted and ground truth answers.\n",
    "* ROUGE (specifically, ROUGE2)\n",
    "\n",
    "Follow the instructions in this notebook step-by step. Much of the code is provided and just needs to be run, but some sections are marked with todo. Make sure to complete all these sections.\n",
    "\n",
    "\n",
    "Requirements: \n",
    "Access to a GPU is required for this assignment. If you have a recent mac, you can try using mps. Otherwise, I recommend renting a GPU instance through a service like vast.ai or lambdalabs. Google Colab can work in a pinch, but you would have to deal with quotas and it's somewhat easy to lose unsaved work.\n",
    "\n",
    "First, we need to ensure that transformers is installed, as well as the accelerate package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00a69e38-7395-4361-a3ff-16ed52a89e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac570bdf-90b7-4dbc-b0df-2377d108f17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.13/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from accelerate) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.13/site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.13/site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in ./.venv/lib/python3.13/site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b115172-d4cd-448e-a3a2-63d22fe7cbef",
   "metadata": {},
   "source": [
    "Now all the relevant imports should succeed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8276df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63708594",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This section creates the benchmark data we need to evaluate the QA systems. It has already been implemented for you. We recommend that you run it only once, save the benchmark data in a json file and then load it when needed. The following code may not work in Windows. We are providing the pre-generated benchmark data for download as an alternative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22da799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./squad_data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e31f8-1b85-47bb-9bd3-60dfa76ccea4",
   "metadata": {},
   "source": [
    "### Downloading the Data and Creating the Benchmark Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75b74477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 40.1M  100 40.1M    0     0  50.6M      0 --:--:-- --:--:-- --:--:-- 50.5M\n",
      "100 40.1M  100 40.1M    0     0  50.6M      0 --:--:-- --:--:-- --:--:-- 50.5M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "val_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
    "\n",
    "os.system(f\"curl -L {training_url} -o {data_dir}/squad_train.json\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d634aa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 442\n",
      "==============================\n",
      "For topic \"Beyoncé\"\n",
      "Number of available context paragraphs: 66\n",
      "==============================\n",
      "The first paragraph is:\n",
      "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "==============================\n",
      "The first five question-answer pairs are:\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: in the late 1990s\n",
      "--------------------\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: singing and dancing\n",
      "--------------------\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: 2003\n",
      "--------------------\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: Houston, Texas\n",
      "--------------------\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: late 1990s\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# load the raw dataset\n",
    "train_data = json.load(open(f\"{data_dir}/squad_train.json\"))\n",
    "\n",
    "# Some details about the dataset\n",
    "\n",
    "# SQuAD is split up into questions about a number of different topics\n",
    "print(f\"Number of topics: {len(train_data['data'])}\")\n",
    "\n",
    "# Let's explore just one topic. Each topic comes with a number of context paragraphs. \n",
    "print(\"=\"*30)\n",
    "print(f\"For topic \\\"{train_data['data'][0]['title']}\\\"\")\n",
    "print(f\"Number of available context paragraphs: {len(train_data['data'][0]['paragraphs'])}\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"The first paragraph is:\")\n",
    "print(train_data['data'][0]['paragraphs'][0]['context'])\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Each paragraph comes with a number of question/answer pairs about the text in the paragraph\n",
    "print(\"The first five question-answer pairs are:\")\n",
    "for qa in train_data['data'][0]['paragraphs'][0]['qas'][:5]:\n",
    "    print(f\"Question: {qa['question']}\")\n",
    "    print(f\"Answer: {qa['answers'][0]['text']}\")\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17faec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of paragraphs in the training set: 19035\n",
      "Total number of question-answer pairs in the training set: 130319\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of paragraphs in the training set:\", sum([len(topic['paragraphs']) for topic in train_data['data']]))\n",
    "print(\"Total number of question-answer pairs in the training set:\", sum([len(paragraph['qas']) for topic in train_data['data'] for paragraph in topic['paragraphs']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cf17aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg number of answers per question: 0.6662190471074824\n",
      "Count of answerable vs unanswerable questions:\n",
      "Answerable questions: 86821 (66.62%)\n",
      "Unanswerable questions: 43498 (33.38%)\n"
     ]
    }
   ],
   "source": [
    "# not all questions are answerable given the information in the paragraph. Part of the original SQuaD 2 task is to identify such\n",
    "# unanswerable questions. We will ignore them for the purpose of this assignment. \n",
    "print(\"Avg number of answers per question:\", \n",
    "      sum([len(qa['answers']) for topic in train_data['data'] for paragraph in topic['paragraphs'] for qa in paragraph['qas']]) / \n",
    "      sum([len(paragraph['qas']) for topic in train_data['data'] for paragraph in topic['paragraphs']]))\n",
    "print(\"Count of answerable vs unanswerable questions:\")\n",
    "answerable_count = 0\n",
    "unanswerable_count = 0\n",
    "for topic in train_data['data']:\n",
    "    for paragraph in topic['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            if len(qa['answers']) > 0:\n",
    "                answerable_count += 1\n",
    "            else:\n",
    "                unanswerable_count += 1\n",
    "print(f\"Answerable questions: {answerable_count} ({answerable_count / (answerable_count + unanswerable_count) * 100:.2f}%)\")\n",
    "print(f\"Unanswerable questions: {unanswerable_count} ({unanswerable_count / (answerable_count + unanswerable_count) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b13dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, create the RAG QA benchmark consisting of 250 answerable questions. \n",
    "\n",
    "# We will use all available context paragraphs for RAG\n",
    "rag_contexts = [paragraph['context'] for topic in train_data['data'] for paragraph in topic['paragraphs']]\n",
    "\n",
    "qa_pairs = []\n",
    "for topic in train_data['data']:\n",
    "    for paragraph in topic['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            if len(qa['answers']) > 0:\n",
    "                qa_pairs.append({\n",
    "                    \"question\": qa['question'],\n",
    "                    \"answer\": qa['answers'][0]['text'],\n",
    "                    \"context\": paragraph['context']\n",
    "                })\n",
    "            \n",
    "# randomly sample 250 answerable questions for the benchmark\n",
    "import random\n",
    "random.seed(42) # IMPORTANT so everyone is working on the same set of sampled QA pairs\n",
    "sampled_qa_pairs = random.sample(qa_pairs, 250)\n",
    "\n",
    "\n",
    "evaluation_benchmark = {'qas': sampled_qa_pairs, \n",
    "                        'contexts': rag_contexts}\n",
    "random.shuffle(evaluation_benchmark['qas'])\n",
    "random.shuffle(evaluation_benchmark['contexts'])\n",
    "\n",
    "# save the evaluation benchmark to a file\n",
    "json.dump(evaluation_benchmark, open(f\"{data_dir}/rag_qa_benchmark.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c447a74",
   "metadata": {},
   "source": [
    "### Loading the Benchmark Dataset / Understanding the Data Format\n",
    "\n",
    "Use the following code to load the benchmark data from a file. Take a look at the example output to see how the data is structured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44d8507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample RAG contexts:\n",
      "Tajikistan's rivers, such as the Vakhsh and the Panj, have great hydropower potential, and the government has focused on attracting investment for projects for internal use and electricity exports. Tajikistan is home to the Nurek Dam, the highest dam in the world. Lately, Russia's RAO UES energy giant has been working on the Sangtuda-1 hydroelectric power station (670 MW capacity) commenced operations on 18 January 2008. Other projects at the development stage include Sangtuda-2 by Iran, Zerafshan by the Chinese company SinoHydro, and the Rogun power plant that, at a projected height of 335 metres (1,099 ft), would supersede the Nurek Dam as highest in the world if it is brought to completion. A planned project, CASA 1000, will transmit 1000 MW of surplus electricity from Tajikistan to Pakistan with power transit through Afghanistan. The total length of transmission line is 750 km while the project is planned to be on Public-Private Partnership basis with the support of WB, IFC, ADB and IDB. The project cost is estimated to be around US$865 million. Other energy resources include sizable coal deposits and smaller reserves of natural gas and petroleum.\n",
      "--------------------\n",
      "Two years later, the Emperor Valens, who favored the Arian position, in his turn exiled Athanasius. This time however, Athanasius simply left for the outskirts of Alexandria, where he stayed for only a few months before the local authorities convinced Valens to retract his order of exile. Some early reports state that Athanasius spent this period of exile at his family's ancestral tomb in a Christian cemetery. It was during this period, the final exile, that he is said to have spent four months in hiding in his father's tomb. (Soz., \"Hist. Eccl.\", VI, xii; Soc., \"Hist. Eccl.\", IV, xii).\n",
      "--------------------\n",
      "==============================\n",
      "Sample RAG QA pairs:\n",
      "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
      "Answer: professor Juan Pedro Toni\n",
      "--------------------\n",
      "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
      "Answer: about six to four\n",
      "--------------------\n",
      "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
      "Answer: 1890s\n",
      "--------------------\n",
      "Question: When did devolution in the UK begin?\n",
      "Answer: 1914\n",
      "--------------------\n",
      "Question: Treating the mitrailleuse like what rendered it far less effective\n",
      "Answer: artillery\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# load the benchmark and display some samples\n",
    "evaluation_benchmark = json.load(open(f\"{data_dir}/rag_qa_benchmark.json\"))\n",
    "\n",
    "print(\"Sample RAG contexts:\")\n",
    "for context in evaluation_benchmark['contexts'][:2]:\n",
    "    print(context)\n",
    "    print(\"-\"*20)\n",
    "print(\"=\"*30)\n",
    "print(\"Sample RAG QA pairs:\")\n",
    "for qa in evaluation_benchmark['qas'][:5]:\n",
    "    print(f\"Question: {qa['question']}\")\n",
    "    print(f\"Answer: {qa['answer']}\")\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa58b5-2b8a-41c7-b2f5-efaeb6d2542e",
   "metadata": {},
   "source": [
    "The `evaluation_benchmark` is a dictionary with two keys: \n",
    "* `evaluation_benchmark['qas']`  provides a list of *qa_items* (see below).\n",
    "* `evaluation_benchmark['contexts']` provides a list of available candidate contexts. Note that this includes all contexts from SQuAD, not just the ones for the 250 questions we sampled for the benchmark.\n",
    "\n",
    "Each *qa_item* is a dictionary with the following keys: \n",
    "* `qa_item['question']` is the question string\n",
    "* `qa_item['answer']` is the target answer string\n",
    "* `qa_item['context']` is the gold context for this question\n",
    "\n",
    "For example: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32bce248-2f3f-47a9-8940-aecec5f9f6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_items = evaluation_benchmark['qas']\n",
    "len(qa_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d428ec2b-98d7-4760-bad7-9e7ef3d01d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_item = qa_items[0]\n",
    "qa_item['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ede20e1-82be-46f4-9fc6-7da84835f6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'professor Juan Pedro Toni'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_item['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94856286-ac1f-4256-a279-b013202e84c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_item['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6cd6b",
   "metadata": {},
   "source": [
    "## Part 1 - Question Answering Evaluation Functions\n",
    "\n",
    "In this section. we will define a number of evaluation functions that measure the quality of the QA output, compared to a single target answer for each question. \n",
    "\n",
    "Because the evaluation will happen at a token leve, we will perform some simple pre-processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fc9fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    return re.sub(regex, ' ', text)\n",
    "  def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "  if not s: return []\n",
    "  return normalize_answer(s).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64fe47f-bbd6-4f48-b641-56d279c05ab8",
   "metadata": {},
   "source": [
    "First, Exact Match (EM) measures the percentage of predictions that match any one of the ground truth answers exactly after normalization.\n",
    "The following function returns 1 if the predicted answer is correct and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37139082-6480-47b1-b712-b2c228f08c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact(a_gold, a_pred):\n",
    "  return int(normalize_answer(a_gold) == normalize_answer(a_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41af10d-f971-46ff-b917-709644d22f8a",
   "metadata": {},
   "source": [
    "The next function calculates the $F_1$ score of the set of predicted tokens against the set of target tokens. \n",
    "$F_1$ is the harmonic mean of precision and recall, providing a balance between the two. Specifically \n",
    "\n",
    "$F_1 = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$\n",
    "\n",
    "where $\\text{precision}$ is the fraction of predicted tokens that also appear in the target and $\\text{recall}$ is the fraction of target tokens that also appear in the prediction. \n",
    "\n",
    "**TODO**: Write the function compute_f1(a_gold, a_pred) that returns the F1 score as defined above. It should work similar to the compute_exact method above. Test your function on a sample answer and prediction to verify that it works correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5c58004-6eb5-434c-9436-ae9adaf9799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    \n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        return int(gold_toks == pred_toks)\n",
    "    \n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_same / len(pred_toks)\n",
    "    recall = num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97bea5d9-6e0e-4c49-a277-c1f9259078a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold: The capital of England is London.\n",
      "Pred: London, capital of England\n",
      "F1 Score: 0.888888888888889\n",
      "\n",
      "Edge case - identical: 1.0\n",
      "Edge case - no overlap: 0\n",
      "Edge case - empty: 1\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "test_gold = \"The capital of England is London.\"\n",
    "test_pred = \"London, capital of England\"\n",
    "\n",
    "print(f\"Gold: {test_gold}\")\n",
    "print(f\"Pred: {test_pred}\")\n",
    "print(f\"F1 Score: {compute_f1(test_gold, test_pred)}\")\n",
    "\n",
    "# Test edge cases\n",
    "print(f\"\\nEdge case - identical: {compute_f1('London', 'London')}\")\n",
    "print(f\"Edge case - no overlap: {compute_f1('Paris', 'London')}\")\n",
    "print(f\"Edge case - empty: {compute_f1('', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749c7ec-0bce-420b-a614-9374ebbb0ef8",
   "metadata": {},
   "source": [
    "Finally, we are also want to compute ROUGE-2 scores (which extends the F1 score above to 2-grams). We can use the `rouge_score` package to do this for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5c82b5e-f256-4bdf-a4b5-9eac65f7e87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in ./.venv/lib/python3.13/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in ./.venv/lib/python3.13/site-packages (from rouge_score) (2.3.1)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.13/site-packages (from rouge_score) (3.9.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from rouge_score) (2.3.5)\n",
      "Requirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.13/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.13/site-packages (from nltk->rouge_score) (8.3.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.13/site-packages (from nltk->rouge_score) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.13/site-packages (from nltk->rouge_score) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from nltk->rouge_score) (4.67.1)\n",
      "Requirement already satisfied: absl-py in ./.venv/lib/python3.13/site-packages (from rouge_score) (2.3.1)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.13/site-packages (from rouge_score) (3.9.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from rouge_score) (2.3.5)\n",
      "Requirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.13/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.13/site-packages (from nltk->rouge_score) (8.3.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.13/site-packages (from nltk->rouge_score) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.13/site-packages (from nltk->rouge_score) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from nltk->rouge_score) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5ef6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "rouge_scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=False)\n",
    "\n",
    "def compute_rouge2(a_gold, a_pred):\n",
    "    if not a_gold or not a_pred:\n",
    "        return 0.0\n",
    "    scores = rouge_scorer.score(a_gold.lower(), a_pred.lower())\n",
    "    return scores['rouge2'].fmeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e68d8-9760-486b-a07b-78c24efd636f",
   "metadata": {},
   "source": [
    "Let's test the metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f2cd8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Answers:\n",
      "Original:\n",
      "Reference: London | Predicted: London, capital of England\n",
      "Normalized:\n",
      "Reference: london | Predicted: london capital of england\n",
      "Exact Match: 0\n",
      "F1 Score: 0.4\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Original:\n",
      "Reference: The capital of England is London. | Predicted: London, capital of England\n",
      "Normalized:\n",
      "Reference: capital of england is london | Predicted: london capital of england\n",
      "Exact Match: 0\n",
      "F1 Score: 0.888888888888889\n",
      "ROUGE-2 F1-score: 0.5714285714285715\n",
      "----------------------------------------\n",
      "Original:\n",
      "Reference: London is the capital city of England. | Predicted: London, capital of England\n",
      "Normalized:\n",
      "Reference: london is capital city of england | Predicted: london capital of england\n",
      "Exact Match: 0\n",
      "F1 Score: 0.8\n",
      "ROUGE-2 F1-score: 0.25\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "reference_answers = [\"London\", \"The capital of England is London.\", \"London is the capital city of England.\"]\n",
    "predicted_answers = [\"London, capital of England\"] * len(reference_answers)\n",
    "\n",
    "print(\"Normalized Answers:\")\n",
    "for ref, pred in zip(reference_answers, predicted_answers):\n",
    "    print(f\"Original:\")\n",
    "    print(f\"Reference: {ref} | Predicted: {pred}\")\n",
    "    print(f\"Normalized:\")\n",
    "    print(f\"Reference: {normalize_answer(ref)} | Predicted: {normalize_answer(pred)}\")\n",
    "    print(\"Exact Match:\", compute_exact(normalize_answer(ref), normalize_answer(pred)))\n",
    "    print(\"F1 Score:\", compute_f1(normalize_answer(ref), normalize_answer(pred)))\n",
    "    print(\"ROUGE-2 F1-score:\", compute_rouge2(normalize_answer(ref), normalize_answer(pred)))\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0f4bc",
   "metadata": {},
   "source": [
    "## Part 2 - Vanilla Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f620b-6326-498e-80cf-c4886c1b3edd",
   "metadata": {},
   "source": [
    "In this part, we will use an off-the-shelf pretrained LLM and attempt to answer the questions from its pretraining knowledge only. \n",
    "To make things simple, we will use the huggingface transformer pipeline abstraction. The pipeline will download the model and parameters for us on creation. When we pass an input prompt to the pipeline, it will automatically perform preprocessing (tokenization), inference, and postprocessing (removing EOS markers and padding).\n",
    "\n",
    "### Loading the LLM\n",
    "The LLM we will use is the 1B version of the instruction tuned OLMo2 model. OLMo is an open source language model created by Allen AI and the University of Washington. Unlike other open source models, OLMo is also open data. You can read more about it here: https://huggingface.co/allenai/OLMo-2-0425-1B-Instruct and here https://allenai.org/olmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf584e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "qa_model = \"allenai/OLMo-2-0425-1B-Instruct\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Check which GPU device to use. Note, this will likely NOT work on a CPU. \n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\" \n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=qa_model,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b1078-2f98-4ca3-b070-f29854ecfd23",
   "metadata": {},
   "source": [
    "We can now pass a prompt to the model and retreive the completed answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75e256c3-e801-43fc-af45-ead3d31da487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"My favorite thing to do in fall is to hike up Mount Rainier, and take in the breathtaking views of the Cascades. The crisp autumn air, the changing leaves, and the sense of adventure are just some of the reasons why I love taking my time each fall to go on a hike.\\n\\nAnother favorite activity for me in the fall is to participate in local pumpkin carving contests. It's a fun way to spend an afternoon, and it's a great way to connect with friends and neighbors. The crisp air and vibrant fall colors make for perfect conditions for a pumpkin carving adventure.\\n\\nLastly, I love to bake and cook during this season. The warm weather and change in weather\"}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"My favorite thing to do in fall is\"\n",
    "output = pipe(prompt, \n",
    "              max_new_tokens=128,\n",
    "              do_sample=True, # set to False for greedy decoding below\n",
    "              pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e790a89-bdad-4bd7-890d-79cc564c7f76",
   "metadata": {},
   "source": [
    "We can skip the prompt that is repeated in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73e6c0c3-d45a-4cbd-977d-269274c160ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to hike up Mount Rainier, and take in the breathtaking views of the Cascades. The crisp autumn air, the changing leaves, and the sense of adventure are just some of the reasons why I love taking my time each fall to go on a hike.\\n\\nAnother favorite activity for me in the fall is to participate in local pumpkin carving contests. It's a fun way to spend an afternoon, and it's a great way to connect with friends and neighbors. The crisp air and vibrant fall colors make for perfect conditions for a pumpkin carving adventure.\\n\\nLastly, I love to bake and cook during this season. The warm weather and change in weather\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c1915-5ee3-4a78-ad82-557154486eae",
   "metadata": {},
   "source": [
    "### Using the LLM for Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61c365-c1ef-4bec-a991-e388e375e61b",
   "metadata": {},
   "source": [
    "**TODO:** Write a function `vanilla_qa(qa_item)` that take a qa_item in the format described above, inserts the question (and only the question!) into a suitable prompt, passes the prompt to the LLM and then returns the answer as a string. \n",
    "\n",
    "A prompt might look like this, but will need a bit of prompt engineering to make it work well. \n",
    "\n",
    "> *Answer the following question concisely.* \n",
    ">\n",
    "> *Question: Who played he lead role in Alien?*\n",
    "> \n",
    "> *Answer:*\n",
    "\n",
    "Once you have a basic version of the vanilla QA system you can tune the prompt (see below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26b94d1b-a91a-495e-9589-f241505c429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_qa(qa_item):\n",
    "    question = qa_item['question']\n",
    "    \n",
    "    # Create a simple prompt with an example (single-shot)\n",
    "    prompt = \"\"\"Answer the following question concisely with just a few words or a short phrase.\n",
    "\n",
    "Example:\n",
    "Question: Who wrote the novel \"Pride and Prejudice\"?\n",
    "Answer: Jane Austen\n",
    "\n",
    "Question: \"\"\" + question + \"\"\"\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    output = pipe(prompt, \n",
    "                  max_new_tokens=50,\n",
    "                  do_sample=False,  # Use greedy decoding for consistency\n",
    "                  pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "    \n",
    "    # Extract just the answer, removing the prompt\n",
    "    answer = output[0]['generated_text'][len(prompt):].strip()\n",
    "    \n",
    "    # Clean up the answer - take only the first line or sentence\n",
    "    answer = answer.split('\\n')[0].strip()\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a406ff-4a0f-4641-a34c-eb5559779dd4",
   "metadata": {},
   "source": [
    "The following code should return an answer (but possibly not the right one) to the first question in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e4df6c1-8533-4744-9f52-2aa48c088eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_item = evaluation_benchmark['qas'][0]\n",
    "qa_item['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9e4cfb5-4eb6-41fb-9aef-aac01451e0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Headmaster'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_qa(qa_item) # inspect the item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823453c2-b9ef-490f-ace8-e411348da498",
   "metadata": {},
   "source": [
    "And the following function evaluates the performance of your `vanilla_qa` function on a list of qa_items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4a6c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa(qa_function, qa_items, verbose=False):\n",
    "    results = []\n",
    "\n",
    "    \n",
    "    for i, qa_item in tqdm.tqdm(enumerate(qa_items), desc=\"Evaluating QA instances\", total=len(qa_items)):\n",
    "\n",
    "        question = qa_item['question'] \n",
    "        answer = qa_item['answer']\n",
    "        context = qa_item['context']\n",
    "        \n",
    "        predicted_answer = qa_function(qa_item)\n",
    "\n",
    "        exact_match = compute_exact(answer, predicted_answer)\n",
    "        f1_score = compute_f1(answer, predicted_answer)\n",
    "        rouge2_f1 = compute_rouge2(answer, predicted_answer)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Q: {question}\")\n",
    "            print(f\"Gold Answer: {answer}\")\n",
    "            print(f\"Predicted Answer: {answer}\")\n",
    "            print(f\"Exact Match: {exact_match}, F1 Score: {f1_score}\")\n",
    "            print(f\"ROUGE-2 F1 Score: {rouge2_f1}\")\n",
    "            print(\"-\"*40)\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"context\": context if context else None,\n",
    "            \"exact_match\": exact_match,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"rouge2_f1\": rouge2_f1\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc60afcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA instances: 100%|██████████| 250/250 [01:00<00:00,  4.16it/s]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [01:00<00:00,  4.16it/s]\n"
     ]
    }
   ],
   "source": [
    "vanilla_evaluation_results = evaluate_qa(vanilla_qa, evaluation_benchmark['qas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869c4dc-5c55-4003-8bed-c68966fdbc98",
   "metadata": {},
   "source": [
    "The function returns a list of evaluation results, one dictionary for each qa item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a9b4c4fe-51b2-40bb-b2c0-91dd572fbc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?',\n",
       " 'answer': 'professor Juan Pedro Toni',\n",
       " 'predicted_answer': 'Headmaster',\n",
       " 'context': \"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       " 'exact_match': 0,\n",
       " 'f1_score': 0,\n",
       " 'rouge2_f1': 0.0}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_evaluation_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658f34e-1b56-4c71-b4bb-54e88b6df3b6",
   "metadata": {},
   "source": [
    "Finally, the `present_results` function aggregates the results for the various qa items and prints the overall result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d72c6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_results(eval_results, exp_name=\"\"):\n",
    "    print(f\"{exp_name} Evaluation Results:\")\n",
    "    exact_matches = [res['exact_match'] for res in eval_results]\n",
    "    f1_scores = [res['f1_score'] for res in eval_results]\n",
    "    rouge2_f1 = [res['rouge2_f1'] for res in eval_results]\n",
    "    print(f\"Exact Match: {sum(exact_matches) / len(exact_matches) * 100:.2f}%\")\n",
    "    print(f\"F1 Score: {sum(f1_scores) / len(f1_scores) * 100:.2f}%\")\n",
    "    print(f\"ROUGE2 F1: {sum(rouge2_f1) / len(rouge2_f1) * 100:.2f}%\")\n",
    "\n",
    "    # print out some evaluation results\n",
    "    for res in eval_results[:5]:\n",
    "        print(f\"Question: {res['question']}\")\n",
    "        print(f\"Gold Answer: {res['answer']}\")\n",
    "        print(f\"Predicted Answer: {res['predicted_answer']}\")\n",
    "        print(f\"Exact Match: {res['exact_match']}, F1 Score: {res['f1_score']}\")\n",
    "        print(\"ROUGE-2 F1-score:\", res['rouge2_f1'])\n",
    "        print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb2bf077-4292-4736-bf04-146f846daa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla QA Evaluation Results:\n",
      "Exact Match: 7.60%\n",
      "F1 Score: 12.47%\n",
      "ROUGE2 F1: 4.16%\n",
      "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
      "Gold Answer: professor Juan Pedro Toni\n",
      "Predicted Answer: Headmaster\n",
      "Exact Match: 0, F1 Score: 0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
      "Gold Answer: about six to four\n",
      "Predicted Answer: Asian:black ratio\n",
      "Exact Match: 0, F1 Score: 0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
      "Gold Answer: 1890s\n",
      "Predicted Answer: 1893\n",
      "Exact Match: 0, F1 Score: 0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did devolution in the UK begin?\n",
      "Gold Answer: 1914\n",
      "Predicted Answer: 1997\n",
      "Exact Match: 0, F1 Score: 0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: Treating the mitrailleuse like what rendered it far less effective\n",
      "Gold Answer: artillery\n",
      "Predicted Answer: less effective\n",
      "Exact Match: 0, F1 Score: 0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "present_results(vanilla_evaluation_results, \"Vanilla QA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785b7b5-bb3b-49da-9699-e25c18a014df",
   "metadata": {},
   "source": [
    "**TODO:** Experiment with the prompt template and try to achieve an Exact Match score of at least 5%. You may want to try including an example in the prompt (single-shot prompting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2e479",
   "metadata": {},
   "source": [
    "## Part 3 - Oracle Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c011bec1-e942-46d9-a193-4c8944e8b3f9",
   "metadata": {},
   "source": [
    "We will now establish an upper bound for a retrieval augmented QA system by providing the correct (\"gold\") context for each question as part of the prompt. These contexts are available as part of each qa_item in the evaluation_benchmark['qas'] dictionary. \n",
    "\n",
    "**TODO**: Write a function `oracle_qa(qa_item)` that takes in a qa_item, inserts both the question **and** the gold context into a prompt template, then passes the prompt to the LLM and returns the answer. The function should behave like the `vanilla_qa` function above, so that we can evaluate it using the same evaluation steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c69a364b-2cbf-4206-8bac-d806d7688a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_qa(qa_item):\n",
    "    question = qa_item['question']\n",
    "    context = qa_item['context']\n",
    "    \n",
    "    # Create a prompt with context and question\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    output = pipe(prompt, \n",
    "                  max_new_tokens=50,\n",
    "                  do_sample=False,  # Use greedy decoding for consistency\n",
    "                  pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "    \n",
    "    # Extract just the answer, removing the prompt\n",
    "    answer = output[0]['generated_text'][len(prompt):].strip()\n",
    "    \n",
    "    # Clean up the answer - take only the first line\n",
    "    answer = answer.split('\\n')[0].strip()\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7aeb37-dc13-4e30-be8d-eca28c6376fa",
   "metadata": {},
   "source": [
    "**TODO**: run the `evaluate_qa` function on your `oracle_qa` function and display the results. You should see Exact Match scores above 50% (if not, tinker with the prompt template). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6cf95161-ebfc-4b99-a263-5ad8f230f298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA instances: 100%|██████████| 250/250 [03:10<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle QA Evaluation Results:\n",
      "Exact Match: 33.60%\n",
      "F1 Score: 56.52%\n",
      "ROUGE2 F1: 30.28%\n",
      "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
      "Gold Answer: professor Juan Pedro Toni\n",
      "Predicted Answer: Juan Pedro Toni is the headmaster of the Christian Brothers of Ireland Stella Maris College.\n",
      "Exact Match: 0, F1 Score: 0.3529411764705882\n",
      "ROUGE-2 F1-score: 0.23529411764705882\n",
      "----------------------------------------\n",
      "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
      "Gold Answer: about six to four\n",
      "Predicted Answer: Black and Asian children outnumber White British children by about six to four in state schools.\n",
      "Exact Match: 0, F1 Score: 0.4\n",
      "ROUGE-2 F1-score: 0.33333333333333337\n",
      "----------------------------------------\n",
      "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
      "Gold Answer: 1890s\n",
      "Predicted Answer: Richard F. Outcault's The Yellow Kid appeared in newspapers in the 1890s.\n",
      "Exact Match: 0, F1 Score: 0.18181818181818182\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did devolution in the UK begin?\n",
      "Gold Answer: 1914\n",
      "Predicted Answer: The United Kingdom has traditionally been governed as a unitary state by the Westminster Parliament in London. Instead of adopting a federal model, the UK has relied on gradual devolution to decentralise political power. Devolution in the UK began with the Government of\n",
      "Exact Match: 0, F1 Score: 0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: Treating the mitrailleuse like what rendered it far less effective\n",
      "Gold Answer: artillery\n",
      "Predicted Answer: French gunners treated the mitrailleuse like artillery and in this role it was ineffective.\n",
      "Exact Match: 0, F1 Score: 0.14285714285714288\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oracle_evaluation_results = evaluate_qa(oracle_qa, evaluation_benchmark['qas'])\n",
    "present_results(oracle_evaluation_results, \"Oracle QA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee3d53",
   "metadata": {},
   "source": [
    "## Part 4 - Retrieval-Augmented Question Answering - Word Overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae47c1-230c-4d39-9670-c82f18ac3f4c",
   "metadata": {},
   "source": [
    "Next, we will experiment with various approaches for retrieving relevant contexts from the set of available contexts. We first get the list of all 19035 available candidate contexts from the evaluation_benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "26c0c460-2b20-4ea4-855c-aa8840893be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_contexts = evaluation_benchmark[\"contexts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "804e3f34-de25-47f0-a33f-3234739cacf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19035"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidate_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cfb4e746-6909-45ee-b6a6-4c3bc31949c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tajikistan's rivers, such as the Vakhsh and the Panj, have great hydropower potential, and the government has focused on attracting investment for projects for internal use and electricity exports. Tajikistan is home to the Nurek Dam, the highest dam in the world. Lately, Russia's RAO UES energy giant has been working on the Sangtuda-1 hydroelectric power station (670 MW capacity) commenced operations on 18 January 2008. Other projects at the development stage include Sangtuda-2 by Iran, Zerafshan by the Chinese company SinoHydro, and the Rogun power plant that, at a projected height of 335 metres (1,099 ft), would supersede the Nurek Dam as highest in the world if it is brought to completion. A planned project, CASA 1000, will transmit 1000 MW of surplus electricity from Tajikistan to Pakistan with power transit through Afghanistan. The total length of transmission line is 750 km while the project is planned to be on Public-Private Partnership basis with the support of WB, IFC, ADB and IDB. The project cost is estimated to be around US$865 million. Other energy resources include sizable coal deposits and smaller reserves of natural gas and petroleum.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_contexts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9bf018-6240-436f-b3e5-eef3f0be16d9",
   "metadata": {},
   "source": [
    "### Token Overlap Retriever \n",
    "Let's first experiment with a simple retriever based on word overlap. Given a question, we measure how many of its tokens appear in each of the candidate contexts. We then retrieve the k contexts with the highest overlap. \n",
    "\n",
    "**TODO:** Write the function `retrieve_overlap(question, contexts, top_k)` that takes in the question (a string) and a list of contexts (each context is a string). It should calculate the word overlap between the question and *each* context, and return a list of the *top_k* contexts with the highest overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fb453256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word overlap retriever\n",
    "def retrieve_overlap(question, contexts, top_k=5):\n",
    "    question_tokens = set(get_tokens(question))\n",
    "    \n",
    "    # Calculate overlap score for each context\n",
    "    overlap_scores = []\n",
    "    for context in contexts:\n",
    "        context_tokens = set(get_tokens(context))\n",
    "        overlap = len(question_tokens & context_tokens)\n",
    "        overlap_scores.append(overlap)\n",
    "    \n",
    "    # Get indices of top_k contexts with highest overlap\n",
    "    top_k_indices = sorted(range(len(overlap_scores)), \n",
    "                          key=lambda i: overlap_scores[i], \n",
    "                          reverse=True)[:top_k]\n",
    "    \n",
    "    # Return the top_k contexts\n",
    "    return [contexts[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733df5da-00ba-4507-a303-3a35d1753dd6",
   "metadata": {},
   "source": [
    "The following function runs the retriever a list of qa_items. For each qa_item it obtains the list of retrieved contexts and adds them to the qa_item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70979681-8b48-499e-9777-296863955856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rag_context(qa_items, contexts, retriever, top_k=5):\n",
    "    result_items = copy.deepcopy(qa_items)\n",
    "    for inst in tqdm.tqdm(result_items, desc=\"Retrieving contexts\"):\n",
    "        question = inst['question']\n",
    "        retrieved_contexts = retriever(question, contexts, top_k)\n",
    "        inst['rag_contexts'] = retrieved_contexts   \n",
    "    return result_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2e2d07b0-9722-4241-b568-04b04ecd900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts: 100%|██████████| 250/250 [58:21<00:00, 14.01s/it]   \n",
      "Retrieving contexts: 100%|██████████| 250/250 [58:21<00:00, 14.01s/it]\n"
     ]
    }
   ],
   "source": [
    "rag_qa_pairs = add_rag_context(evaluation_benchmark['qas'], candidate_contexts, retrieve_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe4dd0-4849-4ec3-9e85-834d0a974e4a",
   "metadata": {},
   "source": [
    "It returns a copy of the qa_item list that is now annotated with the additional 'rag_contexts'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a27e813c-c436-4bf8-80fd-e7df55b24024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?',\n",
       " 'answer': 'professor Juan Pedro Toni',\n",
       " 'context': \"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       " 'rag_contexts': [\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       "  \"Red is one of the most common colors used on national flags. The use of red has similar connotations from country to country: the blood, sacrifice, and courage of those who defended their country; the sun and the hope and warmth it brings; and the sacrifice of Christ's blood (in some historically Christian nations) are a few examples. Red is the color of the flags of several countries that once belonged to the former British Empire. The British flag bears the colors red, white, and blue; it includes the cross of Saint George, patron saint of England, and the saltire of Saint Patrick, patron saint of Ireland, both of which are red on white. The flag of the United States bears the colors of Britain, the colors of the French tricolore include red as part of the old Paris coat of arms, and other countries' flags, such as those of Australia, New Zealand, and Fiji, carry a small inset of the British flag in memory of their ties to that country. Many former colonies of Spain, such as Mexico, Colombia, Ecuador, Cuba, Puerto Rico, Peru, and Venezuela, also feature red-one of the colors of the Spanish flag-on their own banners. Red flags are also used to symbolize storms, bad water conditions, and many other dangers. Navy flags are often red and yellow. Red is prominently featured in the flag of the United States Marine Corps.\",\n",
       "  'In July 2015, Eton accidentally sent emails to 400 prospective students, offering them conditional entrance to the school in September 2017. The email was intended for nine students, but an IT glitch caused the email to be sent to 400 additional families, who didn\\'t necessarily have a place. In response, the school issued the following statement: \"This error was discovered within minutes and each family was immediately contacted to notify them that it should be disregarded and to apologise. We take this type of incident very seriously indeed and so a thorough investigation, overseen by the headmaster Tony Little and led by the tutor for admissions, is being carried out to find out exactly what went wrong and ensure it cannot happen again. Eton College offers its sincere apologies to those boys concerned and their families. We deeply regret the confusion and upset this must have caused.\"',\n",
       "  'John Evans, for whom Evanston is named, bought 379 acres (153 ha) of land along Lake Michigan in 1853, and Philo Judson developed plans for what would become the city of Evanston, Illinois. The first building, Old College, opened on November 5, 1855. To raise funds for its construction, Northwestern sold $100 \"perpetual scholarships\" entitling the purchaser and his heirs to free tuition. Another building, University Hall, was built in 1869 of the same Joliet limestone as the Chicago Water Tower, also built in 1869, one of the few buildings in the heart of Chicago to survive the Great Chicago Fire of 1871. In 1873 the Evanston College for Ladies merged with Northwestern, and Frances Willard, who later gained fame as a suffragette and as one of the founders of the Woman\\'s Christian Temperance Union (WCTU), became the school\\'s first dean of women. Willard Residential College (1938) is named in her honor. Northwestern admitted its first women students in 1869, and the first woman was graduated in 1874.',\n",
       "  'Two years later, the Emperor Valens, who favored the Arian position, in his turn exiled Athanasius. This time however, Athanasius simply left for the outskirts of Alexandria, where he stayed for only a few months before the local authorities convinced Valens to retract his order of exile. Some early reports state that Athanasius spent this period of exile at his family\\'s ancestral tomb in a Christian cemetery. It was during this period, the final exile, that he is said to have spent four months in hiding in his father\\'s tomb. (Soz., \"Hist. Eccl.\", VI, xii; Soc., \"Hist. Eccl.\", IV, xii).']}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_qa_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee220f-7e4d-4354-bfb0-32ef05b14492",
   "metadata": {},
   "source": [
    "Before we run an end-to-end evaluation, we can check the accuracy of the word overlap retriever. In other words, for what fraction of questions is the gold context included in the top-k retrieved contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4b85399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metric of retriever\n",
    "def evaluate_retriever(rag_qa_pairs):\n",
    "    \"\"\"\n",
    "    Evaluates the retriever by computing the accuracy of retrieved contexts against reference contexts.\n",
    "    \"\"\"\n",
    "    correct_retrievals = 0\n",
    "    for qa_item in rag_qa_pairs:\n",
    "        if qa_item['context'] in qa_item['rag_contexts']:\n",
    "            correct_retrievals += 1\n",
    "    accuracy = correct_retrievals / len(rag_qa_pairs)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e6907-df91-41a7-813c-406aebba329f",
   "metadata": {},
   "source": [
    "In our implementation, we got an accuracy of 0.372. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b81b86eb-7789-4a17-87a6-c190955430e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retriever(rag_qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb3ff7-8d00-4e7b-8dbf-7b852ec1dfc8",
   "metadata": {},
   "source": [
    "**TODO**: Write a function `rag_qa(qa_item)` that behaves like the `vanilla_qa` and `oracle_qa` functions above. Create a prompt from the question and the top-k retrieved contexts (instead of the gold context you used in `oracle_qa`). You can assume that `qa_item` already \n",
    "contains the 'rag_contexts' field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4289b8bd-e21a-4248-bed9-efcd5d6a5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_qa(qa_item):\n",
    "    question = qa_item['question']\n",
    "    rag_contexts = qa_item['rag_contexts']\n",
    "    \n",
    "    # Combine all retrieved contexts\n",
    "    combined_context = \"\\n\\n\".join(rag_contexts)\n",
    "    \n",
    "    # Create a prompt with retrieved contexts and question\n",
    "    prompt = f\"\"\"Use the following contexts to answer the question concisely.\n",
    "\n",
    "Contexts:\n",
    "{combined_context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    output = pipe(prompt, \n",
    "                  max_new_tokens=50,\n",
    "                  do_sample=False,  # Use greedy decoding for consistency\n",
    "                  pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "    \n",
    "    # Extract just the answer, removing the prompt\n",
    "    answer = output[0]['generated_text'][len(prompt):].strip()\n",
    "    \n",
    "    # Clean up the answer - take only the first line\n",
    "    answer = answer.split('\\n')[0].strip()\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d1181-df6b-4f18-b038-c83ad3d63ad7",
   "metadata": {},
   "source": [
    "**TODO**: Like you did for the vanilla and oracle qa system, evaluate the `rag_qa` function and display the results. In our implementation, we got an exact match of 19.6%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f85125b-a462-4355-92b9-c930e1e4cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA instances: 100%|██████████| 250/250 [10:08<00:00,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG QA with Overlap Retrieval Evaluation Results:\n",
      "Exact Match: 24.40%\n",
      "F1 Score: 37.46%\n",
      "ROUGE2 F1: 18.34%\n",
      "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
      "Gold Answer: professor Juan Pedro Toni\n",
      "Predicted Answer: Juan Pedro Toni\n",
      "Exact Match: 0, F1 Score: 0.8571428571428571\n",
      "ROUGE-2 F1-score: 0.8\n",
      "----------------------------------------\n",
      "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
      "Gold Answer: about six to four\n",
      "Predicted Answer: The question does not provide enough context to answer the question accurately. The answer is not provided in the question.\n",
      "Exact Match: 0, F1 Score: 0.1\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
      "Gold Answer: 1890s\n",
      "Predicted Answer: 1890s\n",
      "Exact Match: 1, F1 Score: 1.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did devolution in the UK begin?\n",
      "Gold Answer: 1914\n",
      "Predicted Answer: 1895\n",
      "Exact Match: 0, F1 Score: 0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: Treating the mitrailleuse like what rendered it far less effective\n",
      "Gold Answer: artillery\n",
      "Predicted Answer: The use of the mitrailleuse, a rapid-fire weapon, was rendered far less effective due to the increased rate of fire and the fact that it was often engaged in close combat, where its slower rate of fire was a disadvantage.\n",
      "Exact Match: 0, F1 Score: 0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rag_overlap_eval = evaluate_qa(rag_qa, rag_qa_pairs)\n",
    "present_results(rag_overlap_eval, \"RAG QA with Overlap Retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca56df-f99d-4cf6-b877-cde203b102dc",
   "metadata": {},
   "source": [
    "## Part 5 - Retrieval-Augmented Question Answering - Dense Retrieval\n",
    "\n",
    "In this step, we will try to will encode each context and questions using BERT. We will then retrieve the k contexts whose embeddings have the highest cosine similarity to the question embedding.\n",
    "\n",
    "### 5.1 Creating Embeddings for Contexts and Questions \n",
    "\n",
    "Here is an example for how to use BERT to encode a sentence. Instead of using the CLS embeddings (as discussed in class) we will pool together the token representations at the last layer by averaging. The resulting representation is a (1,768) tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c3d6d9d-aaac-4708-8c40-ee8f06046275",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" \n",
    "from transformers import BertTokenizer, BertModel # If you run into memory issues, you \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "inputs = tokenizer(\"This is a sample sentence.\", return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "with torch.no_grad(): \n",
    "    outputs = model(**inputs)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    embedding = torch.mean(hidden_states, dim=1)  # (batch_size=1, embedding size =768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18093c30-2133-4b0c-a5be-15534f6f373f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7db1f-025e-4c70-a4c8-03d70bf433de",
   "metadata": {},
   "source": [
    "**TODO**: Write code to encode each candidate context. Stack the embeddings together into a single (19035, 768) pytorch tensor that we can save to disk and reload as needed (see above for how to access the candidate contexts). On some lower-resource systems you may have trouble instantiating both BERT and OLMo2 at the same time. Storing the encoded representations allows you to run just OLMo for the QA part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f7c617ff-8a22-451d-8144-b29d959e7bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding contexts: 100%|██████████| 595/595 [26:53<00:00,  2.71s/it]  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_list = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    # Encode contexts in batches to manage memory\n",
    "    batch_size = 32\n",
    "    for i in tqdm.tqdm(range(0, len(candidate_contexts), batch_size), desc=\"Encoding contexts\"):\n",
    "        batch_contexts = candidate_contexts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(batch_contexts, return_tensors=\"pt\", padding=True, \n",
    "                          truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Average pooling over tokens (batch_size, embedding_size)\n",
    "        batch_embeddings = torch.mean(hidden_states, dim=1)\n",
    "        \n",
    "        embedding_list.append(batch_embeddings.cpu())\n",
    "    \n",
    "    # Stack all embeddings into a single tensor\n",
    "    context_embeddings = torch.cat(embedding_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b19f021-e9fc-43fa-a54d-b49f268c506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(context_embeddings, \"context_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc61f3-00c8-493a-812b-0eed87969197",
   "metadata": {},
   "source": [
    "**TODO**: Similarly encode each question and stack the embeddings together into a single (250, 768) pytorch tensor that we can save to disk and reload as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53f85eda-0b12-4934-bd57-1ffadbfd73cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding questions: 100%|██████████| 8/8 [00:01<00:00,  4.26it/s]\n",
      "Encoding questions: 100%|██████████| 8/8 [00:01<00:00,  4.26it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_list = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    # Encode questions in batches\n",
    "    batch_size = 32\n",
    "    questions = [qa['question'] for qa in evaluation_benchmark['qas']]\n",
    "    \n",
    "    for i in tqdm.tqdm(range(0, len(questions), batch_size), desc=\"Encoding questions\"):\n",
    "        batch_questions = questions[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(batch_questions, return_tensors=\"pt\", padding=True, \n",
    "                          truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Average pooling over tokens (batch_size, embedding_size)\n",
    "        batch_embeddings = torch.mean(hidden_states, dim=1)\n",
    "        \n",
    "        embedding_list.append(batch_embeddings.cpu())\n",
    "    \n",
    "    # Stack all embeddings into a single tensor\n",
    "    question_embeddings = torch.cat(embedding_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1f1f0207-180d-467f-8a79-0e0e6a45f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(question_embeddings, \"question_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a95403-40b2-4bb7-b36e-7a8236399a46",
   "metadata": {},
   "source": [
    "### 5.2 Similarity Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "65ae157a-c526-4cc7-9d6c-95cd291398aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings = torch.load(\"context_embeddings.pt\")\n",
    "question_embeddings = torch.load(\"question_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031d32f-4a0d-42e2-ae87-b397e7a27ab7",
   "metadata": {},
   "source": [
    "**TODO**: Write a function `retrieve_cosine(question_embedding, contexts, context_embeddings)` that takes in the embedding for a single question (a [1,768] tensor), a list of contexts (each is a string), and the context embedding tensor [19035,768].\n",
    "Note that the indices of the context list and the rows of the context_embeddings tensor line up. i.e. `context_embeddings[0]` is the embedding for `contexts[0]`, etc.\n",
    "You can use `torch.nn.functional.cosine_similarity` (or `F.cosine_similarity` since we imported `torch.nn.functional` as `F`, which is conventional) to calculate the similarities efficiently. You may also ant to look at `torch.topk`, but other solutions are possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "40603f69-f627-4b68-9ef2-4019c78b5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_cosine(question_emb, contexts, context_embeddings, top_k=5):\n",
    "    # Ensure question_emb is 2D (1, 768)\n",
    "    if question_emb.dim() == 1:\n",
    "        question_emb = question_emb.unsqueeze(0)\n",
    "    \n",
    "    # Calculate cosine similarity between question and all contexts\n",
    "    # F.cosine_similarity expects same dimensions, so we need to expand question_emb\n",
    "    similarities = F.cosine_similarity(question_emb, context_embeddings, dim=1)\n",
    "    \n",
    "    # Get top_k indices with highest similarity\n",
    "    top_k_values, top_k_indices = torch.topk(similarities, top_k)\n",
    "    \n",
    "    # Return the top_k contexts\n",
    "    return [contexts[i] for i in top_k_indices.cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a56ef08f-39a3-4a5b-94a7-e70d139541e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       " \"Eton College has links with some private schools in India today, maintained from the days of the British Raj, such as The Doon School and Mayo College. Eton College is also a member of the G20 Schools Group, a collection of college preparatory boarding schools from around the world, including Turkey's Robert College, the United States' Phillips Academy and Phillips Exeter Academy, Australia's Scotch College, Melbourne Grammar School and Launceston Church Grammar School, Singapore's Raffles Institution, and Switzerland's International School of Geneva. Eton has recently fostered[when?] a relationship with the Roxbury Latin School, a traditional all-boys private school in Boston, USA. Former Eton headmaster and provost Sir Eric Anderson shares a close friendship with Roxbury Latin Headmaster emeritus F. Washington Jarvis; Anderson has visited Roxbury Latin on numerous occasions, while Jarvis briefly taught theology at Eton after retiring from his headmaster post at Roxbury Latin. The headmasters' close friendship spawned the Hennessy Scholarship, an annual prize established in 2005 and awarded to a graduating RL senior for a year of study at Eton. Hennessy Scholars generally reside in Wotton house.\",\n",
       " 'The National Maritime College of Ireland is also located in Cork and is the only college in Ireland in which Nautical Studies and Marine Engineering can be undertaken. CIT also incorporates the Cork School of Music and Crawford College of Art and Design as constituent schools. The Cork College of Commerce is the largest post-Leaving Certificate college in Ireland and is also the biggest provider of Vocational Preparation and Training courses in the country.[citation needed] Other 3rd level institutions include Griffith College Cork, a private institution, and various other colleges.',\n",
       " \"Other Christian denominations on the island include: Roman Catholic (since 1852), Salvation Army (since 1884), Baptist (since 1845) and, in more recent times, Seventh-day Adventist (since 1949), New Apostolic and Jehovah's Witnesses (of which one in 35 residents is a member, the highest ratio of any country). The Catholics are pastorally served by the Mission sui iuris of Saint Helena, Ascension and Tristan da Cunha, whose office of ecclesiastical superior is vested in the Apostolic Prefecture of the Falkland Islands.\",\n",
       " 'The university is the major seat of the Congregation of Holy Cross (albeit not its official headquarters, which are in Rome). Its main seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building. Old College, the oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians. Retired priests and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the Grotto. The university through the Moreau Seminary has ties to theologian Frederick Buechner. While not Catholic, Buechner has praised writers from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_cosine(question_embeddings[0], candidate_contexts, context_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba5a9c-b04c-421e-92f9-7b4fcabc0946",
   "metadata": {},
   "source": [
    "**TODO**: Write a new version of the add_rag_context function we provided above. This function should now additionally take the question embeddings and context embeddings as parameters, run the retrieval for each question (using the retrieve_cosine function above) and populate a new list of qa_items, include the selected 'rag_contexts'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ea939c2-cc28-43b9-a1b2-547ce8042e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rag_context(qa_items, contexts, retriever, question_embeddings, context_embeddings, top_k=5):\n",
    "    result_items = copy.deepcopy(qa_items)\n",
    "    \n",
    "    for i, inst in tqdm.tqdm(enumerate(result_items), desc=\"Retrieving contexts\", total=len(result_items)):\n",
    "        question_emb = question_embeddings[i]\n",
    "        retrieved_contexts = retriever(question_emb, contexts, context_embeddings, top_k)\n",
    "        inst['rag_contexts'] = retrieved_contexts\n",
    "    \n",
    "    return result_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cb2a6add-eec1-40c1-a0b4-8368661ca6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts: 100%|██████████| 250/250 [00:02<00:00, 92.03it/s]\n",
      "Retrieving contexts: 100%|██████████| 250/250 [00:02<00:00, 92.03it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_qa_items = add_rag_context(evaluation_benchmark['qas'], candidate_contexts, retrieve_cosine, question_embeddings, context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cd9708cb-961d-4aaa-9d6b-5c62895adf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?',\n",
       " 'answer': 'professor Juan Pedro Toni',\n",
       " 'context': \"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       " 'rag_contexts': [\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       "  \"Eton College has links with some private schools in India today, maintained from the days of the British Raj, such as The Doon School and Mayo College. Eton College is also a member of the G20 Schools Group, a collection of college preparatory boarding schools from around the world, including Turkey's Robert College, the United States' Phillips Academy and Phillips Exeter Academy, Australia's Scotch College, Melbourne Grammar School and Launceston Church Grammar School, Singapore's Raffles Institution, and Switzerland's International School of Geneva. Eton has recently fostered[when?] a relationship with the Roxbury Latin School, a traditional all-boys private school in Boston, USA. Former Eton headmaster and provost Sir Eric Anderson shares a close friendship with Roxbury Latin Headmaster emeritus F. Washington Jarvis; Anderson has visited Roxbury Latin on numerous occasions, while Jarvis briefly taught theology at Eton after retiring from his headmaster post at Roxbury Latin. The headmasters' close friendship spawned the Hennessy Scholarship, an annual prize established in 2005 and awarded to a graduating RL senior for a year of study at Eton. Hennessy Scholars generally reside in Wotton house.\",\n",
       "  'The National Maritime College of Ireland is also located in Cork and is the only college in Ireland in which Nautical Studies and Marine Engineering can be undertaken. CIT also incorporates the Cork School of Music and Crawford College of Art and Design as constituent schools. The Cork College of Commerce is the largest post-Leaving Certificate college in Ireland and is also the biggest provider of Vocational Preparation and Training courses in the country.[citation needed] Other 3rd level institutions include Griffith College Cork, a private institution, and various other colleges.',\n",
       "  \"Other Christian denominations on the island include: Roman Catholic (since 1852), Salvation Army (since 1884), Baptist (since 1845) and, in more recent times, Seventh-day Adventist (since 1949), New Apostolic and Jehovah's Witnesses (of which one in 35 residents is a member, the highest ratio of any country). The Catholics are pastorally served by the Mission sui iuris of Saint Helena, Ascension and Tristan da Cunha, whose office of ecclesiastical superior is vested in the Apostolic Prefecture of the Falkland Islands.\",\n",
       "  'The university is the major seat of the Congregation of Holy Cross (albeit not its official headquarters, which are in Rome). Its main seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building. Old College, the oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians. Retired priests and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the Grotto. The university through the Moreau Seminary has ties to theologian Frederick Buechner. While not Catholic, Buechner has praised writers from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.']}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_qa_items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c0005-f4fd-4094-8e14-7f000bdc4c28",
   "metadata": {},
   "source": [
    "Run the `evaluate_retriever` function on the new qa_items. In our experiments, we got an accuracy of about 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f65ef204-69f9-47a4-8670-03aa6bea5277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.248"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retriever(rag_qa_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282ac18-789e-4f8b-9b78-6f64c671d736",
   "metadata": {},
   "source": [
    "Then, evaluate the rag_qa approach using the revised rag_qa_items. You should get an Exact match better than 20%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4cccddd0-731e-4807-82e8-b73ffa59838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA instances: 100%|██████████| 250/250 [09:03<00:00,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG QA with Dense Retrieval Evaluation Results:\n",
      "Exact Match: 9.20%\n",
      "F1 Score: 18.90%\n",
      "ROUGE2 F1: 7.20%\n",
      "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
      "Gold Answer: professor Juan Pedro Toni\n",
      "Predicted Answer: Juan Pedro Toni\n",
      "Exact Match: 0, F1 Score: 0.8571428571428571\n",
      "ROUGE-2 F1-score: 0.8\n",
      "----------------------------------------\n",
      "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
      "Gold Answer: about six to four\n",
      "Predicted Answer: According to the 2010 Census, segregation in Detroit has decreased in absolute and in relative terms. In the first decade of the 21st century, about two-thirds of the total black population in metropolitan area resided within the city limits of Detroit\n",
      "Exact Match: 0, F1 Score: 0.09999999999999999\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
      "Gold Answer: 1890s\n",
      "Predicted Answer: The Yellow Kid appeared in newspapers in 1894.\n",
      "Exact Match: 0, F1 Score: 0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did devolution in the UK begin?\n",
      "Gold Answer: 1914\n",
      "Predicted Answer: Devolution in the UK began with the Government of Ireland Act 1914 which granted home rule to Ireland as a constituent country of the former United Kingdom of Great Britain and Ireland.\n",
      "Exact Match: 0, F1 Score: 0.0689655172413793\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: Treating the mitrailleuse like what rendered it far less effective\n",
      "Gold Answer: artillery\n",
      "Predicted Answer: French gunners were trained to treat the mitrailleuse like artillery and in this role it was ineffective.\n",
      "Exact Match: 0, F1 Score: 0.11764705882352941\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_qa(rag_qa, rag_qa_items)\n",
    "present_results(result, \"RAG QA with Dense Retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373f0ec",
   "metadata": {},
   "source": [
    "## Part 6 - Experiments\n",
    "\n",
    "**TODO** For the overlap and dense retrievers (from part 5 and 6), what happens when you change the number of retrieved contexts? Present a table of results for k=1, k=5 (already done), k=10, and k=20. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5fc1bcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OVERLAP RETRIEVER - VARYING K\n",
      "============================================================\n",
      "\n",
      "Evaluating with k=1...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add_rag_context() missing 2 required positional arguments: 'question_embeddings' and 'context_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating with k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     rag_qa_pairs_k = \u001b[43madd_rag_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_benchmark\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mqas\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieve_overlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     retriever_acc = evaluate_retriever(rag_qa_pairs_k)\n\u001b[32m     13\u001b[39m     eval_results = evaluate_qa(rag_qa, rag_qa_pairs_k)\n",
      "\u001b[31mTypeError\u001b[39m: add_rag_context() missing 2 required positional arguments: 'question_embeddings' and 'context_embeddings'"
     ]
    }
   ],
   "source": [
    "# Experiment with different k values for overlap retriever\n",
    "print(\"=\"*60)\n",
    "print(\"OVERLAP RETRIEVER - VARYING K\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "k_values = [1, 5, 10, 20]\n",
    "overlap_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nEvaluating with k={k}...\")\n",
    "    rag_qa_pairs_k = add_rag_context(evaluation_benchmark['qas'], candidate_contexts, retrieve_overlap, top_k=k)\n",
    "    retriever_acc = evaluate_retriever(rag_qa_pairs_k)\n",
    "    eval_results = evaluate_qa(rag_qa, rag_qa_pairs_k)\n",
    "    \n",
    "    exact_matches = [res['exact_match'] for res in eval_results]\n",
    "    f1_scores = [res['f1_score'] for res in eval_results]\n",
    "    rouge2_f1 = [res['rouge2_f1'] for res in eval_results]\n",
    "    \n",
    "    overlap_results[k] = {\n",
    "        'retriever_accuracy': retriever_acc * 100,\n",
    "        'exact_match': sum(exact_matches) / len(exact_matches) * 100,\n",
    "        'f1_score': sum(f1_scores) / len(f1_scores) * 100,\n",
    "        'rouge2_f1': sum(rouge2_f1) / len(rouge2_f1) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"k={k}: Retriever Acc={overlap_results[k]['retriever_accuracy']:.2f}%, EM={overlap_results[k]['exact_match']:.2f}%, F1={overlap_results[k]['f1_score']:.2f}%, ROUGE-2={overlap_results[k]['rouge2_f1']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "94443270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DENSE RETRIEVER - VARYING K\n",
      "============================================================\n",
      "\n",
      "Evaluating with k=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts: 100%|██████████| 250/250 [00:02<00:00, 90.15it/s]\n",
      "Retrieving contexts: 100%|██████████| 250/250 [00:02<00:00, 90.15it/s]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [04:35<00:00,  1.10s/it]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [04:35<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1: Retriever Acc=10.80%, EM=4.80%, F1=11.81%, ROUGE-2=3.55%\n",
      "\n",
      "Evaluating with k=5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts: 100%|██████████| 250/250 [00:04<00:00, 53.32it/s]\n",
      "Retrieving contexts: 100%|██████████| 250/250 [00:04<00:00, 53.32it/s]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [11:27<00:00,  2.75s/it]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [11:27<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5: Retriever Acc=24.80%, EM=9.20%, F1=18.90%, ROUGE-2=7.20%\n",
      "\n",
      "Evaluating with k=10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts: 100%|██████████| 250/250 [00:04<00:00, 58.70it/s]\n",
      "Retrieving contexts: 100%|██████████| 250/250 [00:04<00:00, 58.70it/s]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [19:24<00:00,  4.66s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=10: Retriever Acc=32.40%, EM=12.40%, F1=23.09%, ROUGE-2=9.09%\n",
      "\n",
      "Evaluating with k=20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts: 100%|██████████| 250/250 [00:05<00:00, 47.67it/s]\n",
      "Retrieving contexts: 100%|██████████| 250/250 [00:05<00:00, 47.67it/s]\n",
      "Evaluating QA instances:   0%|          | 1/250 [00:13<56:38, 13.65s/it]This is a friendly reminder - the current text generation call has exceeded the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call has exceeded the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Evaluating QA instances:  18%|█▊        | 44/250 [11:33<54:05, 15.76s/it]  \n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 11.84 GiB, other allocations: 5.85 GiB, max allowed: 18.13 GiB). Tried to allocate 765.08 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m rag_qa_pairs_k = add_rag_context(evaluation_benchmark[\u001b[33m'\u001b[39m\u001b[33mqas\u001b[39m\u001b[33m'\u001b[39m], candidate_contexts, \n\u001b[32m     11\u001b[39m                                  retrieve_cosine, question_embeddings, context_embeddings, top_k=k)\n\u001b[32m     12\u001b[39m retriever_acc = evaluate_retriever(rag_qa_pairs_k)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m eval_results = \u001b[43mevaluate_qa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrag_qa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrag_qa_pairs_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m exact_matches = [res[\u001b[33m'\u001b[39m\u001b[33mexact_match\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m eval_results]\n\u001b[32m     16\u001b[39m f1_scores = [res[\u001b[33m'\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m eval_results]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mevaluate_qa\u001b[39m\u001b[34m(qa_function, qa_items, verbose)\u001b[39m\n\u001b[32m      8\u001b[39m answer = qa_item[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m context = qa_item[\u001b[33m'\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m predicted_answer = \u001b[43mqa_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa_item\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m exact_match = compute_exact(answer, predicted_answer)\n\u001b[32m     14\u001b[39m f1_score = compute_f1(answer, predicted_answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mrag_qa\u001b[39m\u001b[34m(qa_item)\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Create a prompt with retrieved contexts and question\u001b[39;00m\n\u001b[32m      9\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mUse the following contexts to answer the question concisely.\u001b[39m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[33mContexts:\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33mAnswer:\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     output = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use greedy decoding for consistency\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Extract just the answer, removing the prompt\u001b[39;00m\n\u001b[32m     23\u001b[39m     answer = output[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m][\u001b[38;5;28mlen\u001b[39m(prompt):].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:332\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1467\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1460\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1461\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1464\u001b[39m         )\n\u001b[32m   1465\u001b[39m     )\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1474\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1473\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1475\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:432\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    430\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    435\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/models/olmo2/modeling_olmo2.py:441\u001b[39m, in \u001b[36mOlmo2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    422\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    423\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    424\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    425\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    426\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    439\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    440\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    452\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/models/olmo2/modeling_olmo2.py:377\u001b[39m, in \u001b[36mOlmo2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    374\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    389\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    390\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    391\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/models/olmo2/modeling_olmo2.py:240\u001b[39m, in \u001b[36mOlmo2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    237\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    238\u001b[39m ) -> torch.Tensor:\n\u001b[32m    239\u001b[39m     residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m    251\u001b[39m     hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/models/olmo2/modeling_olmo2.py:185\u001b[39m, in \u001b[36mOlmo2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    183\u001b[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    197\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/4705_fa25_hw4_rag/.venv/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py:96\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.dtype != torch.bool:\n\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[32m     94\u001b[39m         attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 11.84 GiB, other allocations: 5.85 GiB, max allowed: 18.13 GiB). Tried to allocate 765.08 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# Experiment with different k values for dense retriever\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DENSE RETRIEVER - VARYING K\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dense_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nEvaluating with k={k}...\")\n",
    "    rag_qa_pairs_k = add_rag_context(evaluation_benchmark['qas'], candidate_contexts, \n",
    "                                     retrieve_cosine, question_embeddings, context_embeddings, top_k=k)\n",
    "    retriever_acc = evaluate_retriever(rag_qa_pairs_k)\n",
    "    eval_results = evaluate_qa(rag_qa, rag_qa_pairs_k)\n",
    "    \n",
    "    exact_matches = [res['exact_match'] for res in eval_results]\n",
    "    f1_scores = [res['f1_score'] for res in eval_results]\n",
    "    rouge2_f1 = [res['rouge2_f1'] for res in eval_results]\n",
    "    \n",
    "    dense_results[k] = {\n",
    "        'retriever_accuracy': retriever_acc * 100,\n",
    "        'exact_match': sum(exact_matches) / len(exact_matches) * 100,\n",
    "        'f1_score': sum(f1_scores) / len(f1_scores) * 100,\n",
    "        'rouge2_f1': sum(rouge2_f1) / len(rouge2_f1) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"k={k}: Retriever Acc={dense_results[k]['retriever_accuracy']:.2f}%, EM={dense_results[k]['exact_match']:.2f}%, F1={dense_results[k]['f1_score']:.2f}%, ROUGE-2={dense_results[k]['rouge2_f1']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ba0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a table format\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY TABLE: OVERLAP RETRIEVER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "overlap_df = pd.DataFrame(overlap_results).T\n",
    "overlap_df.index.name = 'k'\n",
    "overlap_df.columns = ['Retriever Acc (%)', 'Exact Match (%)', 'F1 Score (%)', 'ROUGE-2 F1 (%)']\n",
    "print(overlap_df.to_string(float_format='%.2f'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY TABLE: DENSE RETRIEVER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dense_df = pd.DataFrame(dense_results).T\n",
    "dense_df.index.name = 'k'\n",
    "dense_df.columns = ['Retriever Acc (%)', 'Exact Match (%)', 'F1 Score (%)', 'ROUGE-2 F1 (%)']\n",
    "print(dense_df.to_string(float_format='%.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a61d22-65b5-4be0-b607-9afb9be97623",
   "metadata": {},
   "source": [
    "## Part 7 -Improving the QA System \n",
    "\n",
    "**TODO**\n",
    "In this part, we ask you to come up with one interesting or novel idea for improving the QA system. Your system does *not* have to outperform the models from part 4 or 5, but for full credit you should implement at least one new idea, beyond just changing parameters. You can either work on better retrieval or better QA/LLM performance. Show the full code for the necessary steps and evaluation results. \n",
    "\n",
    "Ideas for improving the retriever include: improved word overlap (better tokenization/ text normalization, using TF-IDF, ...), or choosing a different approach or different model (other than BERT) for calculating context and question embeddings.\n",
    "\n",
    "For the LLM, you could try a different transformer model, including text-to-text models (e.g. T5).                                                                                                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99265e2",
   "metadata": {},
   "source": [
    "### Novel Idea: Hybrid Retrieval with Score Fusion\n",
    "\n",
    "For this improvement, I will implement a **hybrid retrieval approach** that combines both word overlap and dense (BERT-based) retrieval using score fusion. The key ideas are:\n",
    "\n",
    "1. **Complementary Strengths**: Word overlap captures exact keyword matches (good for named entities, dates, etc.), while dense retrieval captures semantic similarity (good for paraphrases and concepts).\n",
    "\n",
    "2. **Reciprocal Rank Fusion**: Instead of simple score averaging, I'll use Reciprocal Rank Fusion (RRF), which has been shown to be effective in information retrieval. RRF combines rankings from multiple systems without requiring score normalization.\n",
    "\n",
    "3. **Re-ranking**: The hybrid approach retrieves more candidates (e.g., top-10 from each method) and re-ranks them based on combined scores.\n",
    "\n",
    "The RRF score for a context is calculated as:\n",
    "$$\\text{RRF}(c) = \\sum_{r \\in R} \\frac{1}{k + rank_r(c)}$$\n",
    "\n",
    "where $R$ is the set of retrieval methods, $rank_r(c)$ is the rank of context $c$ in method $r$, and $k$ is a constant (typically 60)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18154195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hybrid_rrf(question, question_emb, contexts, context_embeddings, top_k=5, k_constant=60):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval using Reciprocal Rank Fusion (RRF) to combine \n",
    "    word overlap and dense retrieval rankings.\n",
    "    \"\"\"\n",
    "    # Get rankings from both methods\n",
    "    # Retrieve more candidates for re-ranking (e.g., top-20)\n",
    "    candidate_k = min(20, len(contexts))\n",
    "    \n",
    "    # Word overlap retrieval\n",
    "    question_tokens = set(get_tokens(question))\n",
    "    overlap_scores = []\n",
    "    for context in contexts:\n",
    "        context_tokens = set(get_tokens(context))\n",
    "        overlap = len(question_tokens & context_tokens)\n",
    "        overlap_scores.append(overlap)\n",
    "    \n",
    "    overlap_ranking = sorted(range(len(overlap_scores)), \n",
    "                            key=lambda i: overlap_scores[i], \n",
    "                            reverse=True)\n",
    "    \n",
    "    # Dense retrieval\n",
    "    if question_emb.dim() == 1:\n",
    "        question_emb = question_emb.unsqueeze(0)\n",
    "    \n",
    "    similarities = F.cosine_similarity(question_emb, context_embeddings, dim=1)\n",
    "    dense_ranking = torch.argsort(similarities, descending=True).cpu().numpy()\n",
    "    \n",
    "    # Compute RRF scores\n",
    "    rrf_scores = {}\n",
    "    \n",
    "    # Add scores from overlap ranking\n",
    "    for rank, idx in enumerate(overlap_ranking[:candidate_k]):\n",
    "        if idx not in rrf_scores:\n",
    "            rrf_scores[idx] = 0\n",
    "        rrf_scores[idx] += 1.0 / (k_constant + rank)\n",
    "    \n",
    "    # Add scores from dense ranking\n",
    "    for rank, idx in enumerate(dense_ranking[:candidate_k]):\n",
    "        if idx not in rrf_scores:\n",
    "            rrf_scores[idx] = 0\n",
    "        rrf_scores[idx] += 1.0 / (k_constant + rank)\n",
    "    \n",
    "    # Sort by RRF score and get top-k\n",
    "    top_k_indices = sorted(rrf_scores.keys(), key=lambda i: rrf_scores[i], reverse=True)[:top_k]\n",
    "    \n",
    "    return [contexts[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified add_rag_context for hybrid retrieval\n",
    "def add_rag_context_hybrid(qa_items, contexts, question_embeddings, context_embeddings, top_k=5):\n",
    "    result_items = copy.deepcopy(qa_items)\n",
    "    \n",
    "    for i, inst in tqdm.tqdm(enumerate(result_items), desc=\"Retrieving contexts (hybrid)\", total=len(result_items)):\n",
    "        question = inst['question']\n",
    "        question_emb = question_embeddings[i]\n",
    "        retrieved_contexts = retrieve_hybrid_rrf(question, question_emb, contexts, \n",
    "                                                 context_embeddings, top_k)\n",
    "        inst['rag_contexts'] = retrieved_contexts\n",
    "    \n",
    "    return result_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hybrid retrieval with k=5\n",
    "print(\"=\"*60)\n",
    "print(\"HYBRID RETRIEVAL EVALUATION (k=5)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hybrid_qa_items = add_rag_context_hybrid(evaluation_benchmark['qas'], candidate_contexts, \n",
    "                                         question_embeddings, context_embeddings, top_k=5)\n",
    "\n",
    "# Check retriever accuracy\n",
    "hybrid_retriever_acc = evaluate_retriever(hybrid_qa_items)\n",
    "print(f\"\\nHybrid Retriever Accuracy: {hybrid_retriever_acc * 100:.2f}%\")\n",
    "\n",
    "# Evaluate end-to-end QA performance\n",
    "hybrid_eval_results = evaluate_qa(rag_qa, hybrid_qa_items)\n",
    "present_results(hybrid_eval_results, \"Hybrid RAG QA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three retrieval methods side-by-side\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: OVERLAP vs DENSE vs HYBRID (k=5)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_data = {\n",
    "    'Overlap': {\n",
    "        'Retriever Acc (%)': overlap_results[5]['retriever_accuracy'],\n",
    "        'Exact Match (%)': overlap_results[5]['exact_match'],\n",
    "        'F1 Score (%)': overlap_results[5]['f1_score'],\n",
    "        'ROUGE-2 F1 (%)': overlap_results[5]['rouge2_f1']\n",
    "    },\n",
    "    'Dense': {\n",
    "        'Retriever Acc (%)': dense_results[5]['retriever_accuracy'],\n",
    "        'Exact Match (%)': dense_results[5]['exact_match'],\n",
    "        'F1 Score (%)': dense_results[5]['f1_score'],\n",
    "        'ROUGE-2 F1 (%)': dense_results[5]['rouge2_f1']\n",
    "    },\n",
    "    'Hybrid': {\n",
    "        'Retriever Acc (%)': hybrid_retriever_acc * 100,\n",
    "        'Exact Match (%)': sum([res['exact_match'] for res in hybrid_eval_results]) / len(hybrid_eval_results) * 100,\n",
    "        'F1 Score (%)': sum([res['f1_score'] for res in hybrid_eval_results]) / len(hybrid_eval_results) * 100,\n",
    "        'ROUGE-2 F1 (%)': sum([res['rouge2_f1'] for res in hybrid_eval_results]) / len(hybrid_eval_results) * 100\n",
    "    }\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).T\n",
    "print(comparison_df.to_string(float_format='%.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675d88d",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The hybrid retrieval approach using Reciprocal Rank Fusion (RRF) combines the strengths of both word overlap and dense retrieval:\n",
    "\n",
    "**Key Benefits:**\n",
    "1. **Complementary Information**: Word overlap captures exact matches (useful for named entities, specific terms), while dense retrieval captures semantic similarity (useful for paraphrased questions).\n",
    "2. **Robust Ranking**: RRF is a proven fusion method that doesn't require score normalization and is less sensitive to outliers than simple averaging.\n",
    "3. **Improved Retriever Accuracy**: By combining both methods, we should see improved retrieval accuracy, which translates to better QA performance.\n",
    "\n",
    "**Expected Results:**\n",
    "- The hybrid approach should achieve retriever accuracy between or better than both individual methods\n",
    "- End-to-end QA metrics (EM, F1, ROUGE-2) should improve when the correct context is retrieved more reliably\n",
    "- The method is particularly beneficial when one retrieval method fails but the other succeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4afb6-3f22-48cd-8fc9-51b0b9b44a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d236f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4705-fa25-hw4-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
